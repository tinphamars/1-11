
## üîß C√°c Th√†nh Ph·∫ßn K·ªπ Thu·∫≠t

### 1. Text Splitter

**RecursiveCharacterTextSplitter** chia text th√¥ng minh:

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # K√≠ch th∆∞·ªõc m·ªói chunk
    chunk_overlap=200,      # Overlap ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # ∆Øu ti√™n chia theo paragraph
)
```

**V√≠ d·ª•:**

```
Original text (2500 chars):
"# H·ªá Th·ªëng Qu·∫£n L√Ω\n\n## T·ªïng Quan\nH·ªá th·ªëng gi√∫p...[800 chars]...\n\n## T√≠nh NƒÉng\n### Qu·∫£n L√Ω Nh√¢n Vi√™n\n...[1200 chars]...\n\n## API\nEndpoints ch√≠nh..."

After splitting:
Chunk 1 (1000 chars): "# H·ªá Th·ªëng Qu·∫£n L√Ω\n\n## T·ªïng Quan..."
                        ‚îî‚îÄ overlap 200 chars ‚îÄ‚îê
Chunk 2 (1000 chars):                    "...H·ªá th·ªëng gi√∫p...\n\n## T√≠nh NƒÉng..."
                                          ‚îî‚îÄ overlap 200 chars ‚îÄ‚îê
Chunk 3 (700 chars):                                      "...### Qu·∫£n L√Ω...\n\n## API..."
```

**T·∫°i sao c·∫ßn overlap?**

- Gi·ªØ ng·ªØ c·∫£nh gi·ªØa c√°c chunks
- Tr√°nh m·∫•t th√¥ng tin ·ªü ranh gi·ªõi
- TƒÉng kh·∫£ nƒÉng t√¨m ki·∫øm ch√≠nh x√°c

### 2. Document Loaders

```python
def _get_loader(file_path):
    """Ch·ªçn loader ph√π h·ª£p theo extension"""
    extension = file_path.suffix.lower()
    
    loaders = {
        ".py": PythonLoader,
        ".md": UnstructuredMarkdownLoader,
        ".json": JSONLoader,
        ".docx": UnstructuredWordDocumentLoader,
        ".pdf": PyPDFLoader,
        ".txt": TextLoader,
        ".yaml": TextLoader,
        ".yml": TextLoader,
        ".rst": TextLoader
    }
    
    return loaders.get(extension, TextLoader)(str(file_path))
```

### 3. OpenAI Integration

#### Embeddings

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    openai_api_key=settings.openai_api_key,
    model="text-embedding-ada-002"
)

# Single text
vector = embeddings.embed_query("H·ªá th·ªëng qu·∫£n l√Ω nh√¢n vi√™n")
# ‚Üí [0.023, -0.891, ...] (1536 dimensions)

# Multiple texts
vectors = embeddings.embed_documents([
    "Text 1",
    "Text 2",
    "Text 3"
])
# ‚Üí [[...], [...], [...]]
```

#### Chat Completion

```python
import openai

client = openai.OpenAI(
    base_url="https://aiportalapi.stu-platform.live/jpe",
    api_key=settings.openai_api_key
)

response = client.chat.completions.create(
    model="GPT-4o-mini",
    messages=[
        {"role": "system", "content": "System prompt with context..."},
        {"role": "user", "content": "User question..."}
    ],
    temperature=0.7,
    max_tokens=1000
)

answer = response.choices[0].message.content
```

### 4. ChromaDB Operations

#### Kh·ªüi t·∫°o

```python
import chromadb
from chromadb.config import Settings

client = chromadb.PersistentClient(
    path="./chroma_db",
    settings=Settings(anonymized_telemetry=False)
)

collection = client.get_or_create_collection(
    name="documents",
    metadata={"description": "RAG documents collection"}
)
```

#### Th√™m documents

```python
collection.add(
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],
    documents=["Text 1", "Text 2"],
    metadatas=[{"source": "file1.md"}, {"source": "file2.md"}],
    ids=["doc_0", "doc_1"]
)
```

#### T√¨m ki·∫øm

```python
results = collection.query(
    query_embeddings=[[0.15, 0.25, ...]],
    n_results=5,
    include=["documents", "metadatas", "distances"]
)
```

## ‚ö° T·ªëi ∆Øu H√≥a

### 1. Concurrent Processing

```python
# X·ª≠ l√Ω nhi·ªÅu files song song
tasks = [_process_single_file(file) for file in valid_files]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

### 2. Thread Pool for I/O

```python
executor = ThreadPoolExecutor(max_workers=4)

# Ch·∫°y blocking operations trong thread pool
embeddings = await asyncio.get_event_loop().run_in_executor(
    executor, 
    openai_embeddings.embed_documents, 
    texts
)
```

### 3. Batch Processing

```python
# T·∫°o embeddings theo batch thay v√¨ t·ª´ng c√°i m·ªôt
BATCH_SIZE = 100

for i in range(0, len(documents), BATCH_SIZE):
    batch = documents[i:i + BATCH_SIZE]
    embeddings = create_embeddings(batch)
    save_to_db(embeddings)
```

### 4. Caching

ChromaDB t·ª± ƒë·ªông cache:
- L∆∞u persistent tr√™n disk
- Kh√¥ng c·∫ßn reload khi restart
- T√¨m ki·∫øm nhanh v·ªõi index

## üìä Metrics & Monitoring

### Metrics Quan Tr·ªçng

```python
# Document processing
- documents_processed_total
- documents_failed_total
- chunks_created_total
- processing_time_seconds

# Search
- search_queries_total
- search_latency_seconds
- search_results_count

# Chat
- chat_requests_total
- chat_response_time_seconds
- tokens_used_total
- context_relevance_score
```

### Logging

```python
import logging

logger = logging.getLogger(__name__)

# Info level
logger.info(f"Auto-loaded {count} files, {chunks} chunks")

# Warning level
logger.warning(f"File too large: {file_path}")

# Error level
logger.error(f"Failed to process {file_path}: {error}")
```

## üéØ Best Practices

### 1. Chunk Size Optimization

```
Qu√° nh·ªè (< 500):  M·∫•t ng·ªØ c·∫£nh, nhi·ªÅu chunks kh√¥ng c·∫ßn thi·∫øt
T·ªëi ∆∞u (800-1200): C√¢n b·∫±ng ng·ªØ c·∫£nh v√† ƒë·ªô ch√≠nh x√°c
Qu√° l·ªõn (> 2000):  GPT c√≥ th·ªÉ b·ªã overwhelm, ch·∫≠m
```

### 2. Retrieval K Value

```python
k = 3:  Nhanh, √≠t context, c√≥ th·ªÉ thi·∫øu th√¥ng tin
k = 5:  C√¢n b·∫±ng t·ªët (khuy·∫øn ngh·ªã)
k = 10: Nhi·ªÅu context, nh∆∞ng c√≥ th·ªÉ c√≥ noise
```

### 3. Temperature Settings

```
0.0 - 0.3:  Deterministic, ch√≠nh x√°c, l·∫∑p l·∫°i
0.5 - 0.7:  C√¢n b·∫±ng (khuy·∫øn ngh·ªã cho RAG)
0.8 - 1.0:  S√°ng t·∫°o, nh∆∞ng c√≥ th·ªÉ sai l·ªách
```

### 4. Context Window Management

```
GPT-4o-mini: 128k tokens context window
M·ªói token ‚âà 4 chars

V√≠ d·ª•:
5 documents √ó 1000 chars = 5000 chars ‚âà 1250 tokens
System prompt: ~500 tokens
User message: ~50 tokens
Response budget: ~1000 tokens
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: ~2800 tokens (c√≤n r·∫•t nhi·ªÅu cho context)
```
